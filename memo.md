# 線形回帰 (Liner Regression)

## 重要な概念

* データセット (x, y):
* 仮説関数 hθ(x) = θ0 + θ1x: データxが与えられたときの予測を行うモデル
* パラメータ θ0, θ1: 仮説関数（モデル）のパラメータ
* コストファンクション J(θ): 仮説関数の予想値とトレーニングセットの正解を比較しモデルの質を評価する計算式。もし、0ならトレーニングセットと予測の誤差はないということで、完璧に予測できているということ <= 過学習の可能性ありあり
* Gradient Descent（最急降下法）: minJ(θ)を求めるアルゴリズム

## 流れ

1. データセット（特徴Xとラベルy）を用意する
1.1. 必要なら、feature normalization(feature scalling)（データの正規化）を行う
1.2. トレーニングセット、クロスバリデーション、テストセットと３つに分ける
2. トレーニングセットを使い、仮説関数 hθ(x) のパラメータθを算出する。
2.1. 最適なθを求めるには、コストファンクション J(θ) の最小値を求める（minJ(θ)）
2.2. minJ(θ)を行うには、Gradient Descent Algorithmを使う
3. テストセットでモデル（仮説関数）の精度の評価を行う

## 精度向上

* 分散している（非線形）の場合は、精度は一定以上は上がらないので他のアルゴリズムを使う必要がある。たぶん、ニューラルネットワークなど。
* 2次関数にフィットするようなデータの場合、特徴を増やして2次感数にすることで、精度は向上する。（3次, 4次, ...も同様）
* 外れ値に大きく影響を受けるらしい。トレーニングデータを多くして対応。たぶん。
