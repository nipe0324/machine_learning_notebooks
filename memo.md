# 線形回帰 (Liner Regression)

## 重要な概念

* データセット (x, y):
* 仮説関数 hθ(x) = θ0 + θ1x: データxが与えられたときの予測を行うモデル
* パラメータ θ0, θ1: 仮説関数（モデル）のパラメータ
* コストファンクション J(θ): 仮説関数の予想値とトレーニングセットの正解を比較しモデルの質を評価する計算式。もし、0ならトレーニングセットと予測の誤差はないということで、完璧に予測できているということ <= 過学習の可能性ありあり
* Gradient Descent（最急降下法）: minJ(θ)を求めるアルゴリズム

## 流れ

1. データセット（特徴Xとラベルy）を用意する
1.1. 必要なら、feature normalization(feature scalling)（データの正規化）を行う
1.2. トレーニングセット、クロスバリデーション、テストセットと３つに分ける
2. トレーニングセットを使い、仮説関数 hθ(x) のパラメータθを算出する。
2.1. 最適なθを求めるには、コストファンクション J(θ) の最小値を求める（minJ(θ)）
2.2. minJ(θ)を行うには、Gradient Descent Algorithmを使う
3. テストセットでモデル（仮説関数）の精度の評価を行う

## 精度向上

* 分散している（非線形）の場合は、精度は一定以上は上がらないので他のアルゴリズムを使う必要がある。たぶん、ニューラルネットワークなど。
* 2次関数にフィットするようなデータの場合、特徴を増やして2次感数にすることで、精度は向上する。（3次, 4次, ...も同様）
  * TODO: どういうロジックでPolynomial featuresを導入する？
* 外れ値に大きく影響を受けるらしい。トレーニングデータを多くして対応。たぶん。


# ロジスティック回帰（Logistic Regression）

* Coursera: https://www.coursera.org/learn/machine-learning/home/week/3

## 重要な概念

* 仮説関数 hθ(x) = g(z) = g(θT * x): 閾値 0.5など以上なら1(positive), 以下なら0(negative)と判断する。閾値は適宜変えてOK
* シグモイド関数 g(z) = 1 / 1 + e^-z: 0 <= g(z) <= 1 となる関数
* コストファンクション
* GradientDescent


# Regularization（正規化）

https://www.coursera.org/learn/machine-learning/home/week/3

* オーバーフィッティング（過学習）をしないようにするための手法
* オーバーフィッティングとは、トレーニング時にトレーニングセットにだけフィットしすぎるモデルが作成されてしまい、実運用で精度がでなくなってしまう問題。高次の関数を適用するとオーバーフィッティングになりやすい。でも、データ量を増やせばオーバーフィッティングは徐々に解消される。
